{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Code TensorFlow Wide & Deep TF.Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: argparse in /usr/lib/python2.7\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.1.0->tensorflow)\n",
      "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python2.7/dist-packages (from setuptools->protobuf>=3.1.0->tensorflow)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python2.7/dist-packages (from setuptools->protobuf>=3.1.0->tensorflow)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python2.7/dist-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tensorflow argparse --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preserving warning output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Column Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "LABEL_COLUMN = \"label\"\n",
    "CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "                       \"relationship\", \"race\", \"gender\", \"native_country\"]\n",
    "CONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n",
    "                      \"hours_per_week\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Determine train and test data file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_train_data_files(train_data, test_data):\n",
    "  \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "  if train_data:\n",
    "    train_file_name = train_data\n",
    "  else:\n",
    "    train_file_name = \"train_data.csv\"\n",
    "    print(\"Training data is used from %s\" % train_file_name)\n",
    "\n",
    "  if test_data:\n",
    "    test_file_name = test_data\n",
    "  else:\n",
    "    test_file_name = \"test_data.csv\"\n",
    "    print(\"Test data is used from %s\" % test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building custom-purpose Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  # Sparse base columns.\n",
    "  gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\",\n",
    "                                                     keys=[\"female\", \"male\"])\n",
    "  education = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"education\", hash_bucket_size=1000)\n",
    "  relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"relationship\", hash_bucket_size=100)\n",
    "  workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"workclass\", hash_bucket_size=100)\n",
    "  occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"occupation\", hash_bucket_size=1000)\n",
    "  native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "  # Continuous base columns.\n",
    "\n",
    "  age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "  education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "  capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "  capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "  hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.contrib.layers.bucketized_column(age,\n",
    "                                                    boundaries=[\n",
    "                                                        18, 25, 30, 35, 40, 45,\n",
    "                                                        50, 55, 60, 65\n",
    "                                                    ])\n",
    "\n",
    "  # Wide columns and deep columns.\n",
    "  wide_columns = [gender, native_country, education, occupation, workclass,\n",
    "                  relationship, age_buckets,\n",
    "                  tf.contrib.layers.crossed_column([education, occupation],\n",
    "                                                   hash_bucket_size=int(1e4)),\n",
    "                  tf.contrib.layers.crossed_column(\n",
    "                      [age_buckets, education, occupation],\n",
    "                      hash_bucket_size=int(1e6)),\n",
    "                  tf.contrib.layers.crossed_column([native_country, occupation],\n",
    "                                                   hash_bucket_size=int(1e4))]\n",
    "  deep_columns = [\n",
    "      tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(native_country,\n",
    "                                         dimension=8),\n",
    "      tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "  ]\n",
    "\n",
    "  if model_type == \"wide\":\n",
    "    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n",
    "                                          feature_columns=wide_columns)\n",
    "  elif model_type == \"deep\":\n",
    "    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n",
    "                                       feature_columns=deep_columns,\n",
    "                                       hidden_units=[100, 50])\n",
    "  else:\n",
    "    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])\n",
    "  return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Performing input data transformation to correspond to NN internal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {\n",
    "      k: tf.SparseTensor(\n",
    "          indices=[[i, 0] for i in range(df[k].size)],\n",
    "          values=df[k].values,\n",
    "          dense_shape=[df[k].size, 1])\n",
    "      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Performing training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  print(\"Model is training...\")\n",
    "  train_file_name, test_file_name = get_train_data_files(train_data, test_data)\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "\n",
    "  # remove NaN elements\n",
    "  df_train = df_train.dropna(how='any', axis=0)\n",
    "  df_test = df_test.dropna(how='any', axis=0)\n",
    "\n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "    \n",
    "  model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir, model_type)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)\n",
    "\n",
    "  print(\"Model is evaluating...\")\n",
    "\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  print(df_test)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main fuction (command line arguments parsing and calling train & evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is training...\n",
      "Training data is used from train_data.csv\n",
      "Test data is used from test_data.csv\n",
      "model directory = /tmp/tmp_dYuOv\n",
      "Model is evaluating...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (0,) must have rank 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-26c6b9b3962c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m   )\n\u001b[1;32m     41\u001b[0m   \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-26c6b9b3962c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n\u001b[0;32m----> 5\u001b[0;31m                  FLAGS.train_data, FLAGS.test_data)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-1ecb1ba1b28b>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model_dir, model_type, train_steps, train_data, test_data)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model is evaluating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 280\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    282\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, input_fn, feed_fn, batch_size, steps, metrics, name, checkpoint_path, hooks, log_progress)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         log_progress=log_progress)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_evaluate_model\u001b[0;34m(self, input_fn, steps, feed_fn, metrics, name, checkpoint_path, hooks, log_progress)\u001b[0m\n\u001b[1;32m    798\u001b[0m       \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m       \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrib_framework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-1ecb1ba1b28b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model is evaluating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-09df6662092d>\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           dense_shape=[df[k].size, 1])\n\u001b[0;32m---> 13\u001b[0;31m       for k in CATEGORICAL_COLUMNS}\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Merges the two dictionaries into one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuous_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-09df6662092d>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((k,))\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           dense_shape=[df[k].size, 1])\n\u001b[0;32m---> 13\u001b[0;31m       for k in CATEGORICAL_COLUMNS}\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Merges the two dictionaries into one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuous_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/sparse_tensor.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, indices, values, dense_shape)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mindices_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mvalues_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mdense_shape_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mwith_rank\u001b[0;34m(self, rank)\u001b[0m\n\u001b[1;32m    654\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape %s must have rank %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwith_rank_at_least\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape (0,) must have rank 2"
     ]
    }
   ],
   "source": [
    "FLAGS = None\n",
    "\n",
    "def main(_):\n",
    "  train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "                 FLAGS.train_data, FLAGS.test_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  parser.add_argument(\n",
    "      \"--model_dir\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Base directory for output models.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--model_type\",\n",
    "      type=str,\n",
    "      default=\"wide_n_deep\",\n",
    "      help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--train_steps\",\n",
    "      type=int,\n",
    "      default=200,\n",
    "      help=\"Number of training steps.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--train_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to the training data.\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--test_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to the test data.\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "  sys.exit()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
